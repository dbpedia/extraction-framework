# copy (and re-compress) all .gz files to .bz2 files in background process, write log output to compress.log
for file in $(find -name '*.gz' | sort) ; do echo $file ; gzip -d <$file | bzip2 -c >${file/%.gz/.bz2} ; done &>compress.log &

# pbzip2 is much faster with multiple cores / CPUs
for file in $(find -name '*.gz' | sort) ; do echo $file ; gzip -d <$file | pbzip2 -c >${file/%.gz/.bz2} ; done &>compress.log &

# copy (and re-compress) all files listed in files.txt from .gz to .bz2 files on a separate partition
# in background process, write log output to log file, including timestamps
# pigz may be faster than gzip even for decompression. pbzip2 is much faster with multiple cores / CPUs.
( date ; for f in $(cat files.txt) ; do echo $f ; pigz -d </data/dbpedia-release/3.9/$f.gz | pbzip2 -c >/data2/dbpedia-release/3.9/$f.bz2 ; date ; done ) &> files.log &

# Don't overwrite existing non-empty .bz2 files.
for file in $(find -name '*.gz' | sort) ; do copy=${file/%.gz/.bz2} ; if [[ ! -s $copy ]] ; then echo $file ; gzip -d <$file | pbzip2 -c >$copy ; fi ; done &>compress.log &

# Only generate .bz2 file if it doesn't exist or is older than .gz file.
for file in $(find -name '*.gz' | sort) ; do copy=${file/%.gz/.bz2} ; if [[ "$file" -nt "$copy" ]] ; then echo $file ; gzip -d <$file | pbzip2 -c >$copy ; fi ; done &>compress.log &

# Vice versa: 'copy' all .bz2 files whose .gz companion doesn't exist or is empty 
# using pigz (parallel gzip), in background process, write log output to compress.log
for file in $(find -name '*.bz2' | sort) ; do copy=${file/%.bz2/.gz} ; if [[ ! -s "$copy" ]] ; then echo $file ; bzip2 -d <$file | pigz -c >$copy ; fi ; done &>compress.log &

# find out if all pairs of files (in this case, -skos-categories and -skos-categories-redirected) 
# have almost the same number of bytes (and are therefore almost certainly identical except 
# different dates in header and footer)
find -name *-skos-categories*.nt.gz -ls | sort -k 11 | awk '{if (p == 0) p = $7; else { if (p > $7 + 5 || p < $7 - 5) { print p " " $7 " " $11; } p=0; } }'

# differences between .bz2 files are larger, probably because pbzip2 is a bit different, so relative comparison works better
find -name *-skos-categories*.nt.bz2 -ls | sort -k 11 | awk '{if (p == 0) p = $7; else { if ($7 / p < .99 || p / $7 < .99 ) { print p " " $7 " " $11; } p=0; } }'


# sort a triples file by property (field 2)
# -s means 'stable' - without it, other fields are also sorted, which is a waste of time
sort -s -k 2,2 enwiki-20120601-infobox-properties.ttl >enwiki-20120601-infobox-properties.sorted.ttl

# split a triples file (must be sorted by property name) into separate files for each property
# 29 is the length of the string '<http://dbpedia.org/property/'
# 50 is an arbitrary cutoff to avoid overly long file names
awk '{if ($1 != "#") { if ($2 != last) {last = $2; close(file); counter++; len = length($2) - 29; if (len > 50) len = 50; name = substr($2, 29, len); gsub(/\//, "", name); file = "split-by-property/" sprintf("%.6d", counter) "-" name ".ttl" } print >> file } }' enwiki-20120601-infobox-properties.sorted.ttl
# split by property, use part after last slash as property name. no special handling of comment lines.
awk '{ if ($2 != last) {last = $2; close(file); count = split(substr($2, 2, length($2) - 2), parts, "/"); file = parts[count]".ttl" } print >> file }' xyz.ttl

# delete all but the latest directory for each language and print how many GB were freed / kept
# assumes that the shell sorts the result of */* alphabetically
# run this script in base dir of all languages!
# WARNING: even if the latest directory for a language is empty or almost empty, all other directories 
# for this language will be deleted! You probably don't want this. I sure didn't... :-(
du -B 1 -s */* | awk '{split($2, arr, "/"); if (arr[2] != "index.html") { if (name == arr[1]) { system("rm -fr " dir); print dir " <<< gone"; gone += size } else if (name != "") { print dir " <<< kept"; kept += size } name = arr[1]; size = $1; dir = $2 } } END {print dir " <<< kept"; print gone / 1024 / 1024 / 1024; print kept / 1024 / 1024 / 1024 }'

# for all .gz files, print packed size in bytes, lines, unpacked size in bytes, store in packed-lines-bytes.gz.txt
for file in $(find -name '*.gz' | sort) ; do stat --printf='%n %s ' $file ; gzip -d <$file | wc -cl ; done >packed-lines-bytes.gz.txt
# for all .gz files, print name without leading './' and trailing '.gz', print lines, bytes, gzip size, store in lines-bytes-packed.txt
for file in $(find -name '*.gz' | sort) ; do stat --printf='%n %s ' $file ; gzip -d <$file | wc -cl ; done | awk '{sub(/^\.\//, "", $1); sub(/.gz$/, "", $1); print $1" lines: "$3" bytes: "$4" gzip: "$2}' >lines-bytes-packed.txt
# Mac: for all .nt files, print name, lines, and size of .nt, .nt.gz, .nt.bz2 files
for i in *.nt ; do stat -n -f '%z' $i ; stat -n -f ' %z' $i.gz ; stat -n -f ' %z' $i.bz2 ; wc -l $i ; done | awk '{print $5" lines: "$4" bytes: "$1" gzip: "$2" bzip2: "$3}'
# same for Linux
for i in *.nt ; do stat --printf '%s' $i ; stat --printf ' %s' $i.gz ; stat --printf ' %s ' $i.bz2 ; wc -l $i ; done | awk '{print $5" lines: "$4" bytes: "$1" gzip: "$2" bzip2: "$3}'

# rename all files for which a -redirected dataset now exists to -unredirected
find -mindepth 3 -type f | grep -E '(disambiguations|infobox-properties|mappingbased-properties|page-links|persondata|topical-concepts)' | grep -v -E '(specific-mappingbased|redirected)' | awk '{new=$0; sub(/\.\//, "", new); sub(/\./, "-unredirected.", new); system("echo mv " $0 " " new)}'

# remove "-redirected" from all file names; use --interactive to ask before an existing file is overwritten  
for i in */*/*-redirected* ; do mv --interactive  $i ${i/-redirected/}; done

# create symbolic links for all *.bz2 files from old directory and name structure to new one
find /home/release/wikipedia -maxdepth 3 -name '*.bz2' | grep -v -E 'ontology.owl|pages-articles.xml' | env LC_ALL=C sort | while read path ; do file=${path##*/} ; lang=${file%%wiki-*} ; tail=${file#*-*-} ; name=${tail%%.*} ; suffix=${tail#*.} ; echo $path ; ln -s $path 3.8/$lang/${name//-/_}_$lang.$suffix ; done

# in lines-bytes-packed.txt, convert xxwiki/19871231/xxwiki-19871231-yyy.nt paths to xx/yyy_xx.nt paths  
cat lines-bytes-packed.txt | while read line ; do path=${line%% *} ; rest=${line#* } ; file=${path##*/} ; lang=${file%%wiki-*} ; tail=${file#*-*-} ; name=${tail%%.*} ; suffix=${tail#*.} ; echo $lang/${name//-/_}_$lang.$suffix $rest ; done >lines-bytes-packed.dl.txt

# generate wiki syntax for page like http://wiki.dbpedia.org/DumpDatesDBpedia38
find -maxdepth 2 -mindepth 2 -type d | env LC_ALL=C sort | awk -F / '{sub(/wiki/, "", $2); print "||" $2 "|" substr($3,1,4) "-" substr($3,5,2) "-" substr($3,7,2) "||"}'

# count number of same-as links in Wikipedia article namespace from English to other extracted languages. Result in 3.9: 16802429
gzip -d < enwiki-20130403-interlanguage-links.ttl.gz | grep -v -E 'resource/(Category|Template):' | wc -l

# count number of abstracts, i.e. non-redirect, non-disambig pages in Wikipedia article namespace, in all extracted languages. Result in 3.9: 24895769
grep 'short_abstracts\w*\.ttl' lines-bytes-packed.txt | grep -v 'en_uris' | awk '{s+=$3} END {print s}'

# count mappings (per language and total)
grep -c '<page>' mappings/Mapping_* | sed 's/[_:.]/ /g' | sort -k 4 -n -r | awk '{s+=$4; print $2 " " $4} END {print "total " s}'

# ontology: count classes
grep -c '<owl:Class' ontology.owl 

# ontology: count object properties 
grep -c '<owl:ObjectProperty' ontology.owl

# ontology: count datatype properties, not specialized 
grep -c -E '<owl:DatatypeProperty rdf:about="http://dbpedia.org/ontology/\w+">' ontology.owl

# ontology: count specialized datatype properties
grep -c -E '<owl:DatatypeProperty rdf:about="http://dbpedia.org/ontology/\w+/\w+">' ontology.owl

# remove all lines in file bad.nt from file all.nt, write result to clean.nt
awk '{ if (f==1) { r[$0] } else if (! ($0 in r)) { print $0 } }' f=1 bad.nt f=2 all.nt >clean.nt
# remove all triples in file bad.nt from quad file all.nq
awk '{ if (f==1) { r[$0] } else { quad = $0; NF -= 2; triple = $0" ." ; if (! (triple in r)) { print quad } } }' f=1 bad.nt f=2 all.nq >clean.nq
# remove Windows line breaks first, awk looks only for \n. '\015' is octal for '\d', but is more portable:
tr -d '\015' bad.nt >bad-unix.nt
# remove duplicate bad triples
sort -u bad-unix.nt >bad-sorted.nt

# temporarily open port 8880 for incoming TCP. rule disappears on restart. 
sudo iptables -I INPUT -i eth0 -p tcp --dport 8880 -m state --state NEW,ESTABLISHED -j ACCEPT

# check that all links from the download page to the download server are correct
wget --background --spider --recursive --level 1 --domains=downloads.dbpedia.org --span-hosts http://wiki.dbpedia.org/Downloads39

# Try to decode URIs: 
# sed replaces non-ASCII-escaping '%' by '\x', "read -r" ignores backslash, "echo -e" converts '\xHH' to byte
# Notes:
# - only works when the shell uses UTF-8
# - you should check that only valid escaped ASCII chars occur in the file: %25 %22 %3F %5C %5E %60
# - the others are invalid in Wikipedia titles: %23 %3C %3E %5B %5D %7B %7C %7D 
# - may fail for other reasons, depending on file content, shell settings, etc.
sed -r 's/%([89ABCDEF])/\\x\1/g' abc.nt | while read -r line ; do echo -e $line ; done > abc.ttl

# list all page-links files
ls -1 */*/*-page-links*.{nt,ttl}.gz | grep -v -E 'en-uris|unredirected' > page-links.txt

# generate in-link counts
( export LC_ALL=C ; cat page-links.txt | while read i ; do echo $i; gzip -d < $i | grep -v '^#' | cut -d ' ' -f 3 | sort | uniq -c | sort -k 1,1 -n -s -r | awk '{print $2" <http://dbpedia.org/ontology/wikiPageInLinkCount> \""$1"\"^^<http://www.w3.org/2001/XMLSchema#integer> ." }' | gzip -c > ${i/page-links/page-in-link-counts} ; done &> page-in-link-counts.log & )

# generate out-link count
# (the first sort is not really necessary - our triples are already grouped by subject, and grouping is enough for uniq - but it's nicer that the in-link and out-link counts are sorted similarly)
( export LC_ALL=C ; cat page-links.txt | while read i ; do echo $i; gzip -d < $i | grep -v '^#' | cut -d ' ' -f 1 | sort | uniq -c | sort -k 1,1 -n -s -r | awk '{print $2" <http://dbpedia.org/ontology/wikiPageOutLinkCount> \""$1"\"^^<http://www.w3.org/2001/XMLSchema#integer> ." }' | gzip -c > ${i/page-links/page-out-link-counts} ; done &> page-out-link-counts.log & )
