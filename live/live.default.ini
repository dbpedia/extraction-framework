
;*******************
; Framework default ini file
; overwritten by file dbpedia.ini not checked into the svn
; copy one of the dist.ini to dbpedia.ini
;*******************
timezone = Etc/UTC

; This will be used to setup the default min date for the feeders
uploaded_dump_date = 2012-04-01T15:00:00Z

; The place where application temporary files will be written (not public files)
working_directory = /path/to/live-data/application-data

; path to save the created triples
publishDiffRepoPath=/path/to/live-data/changesets

;annotations are created
;works with SimpleDumpdestination and LiveUpdateDestination
generateOWLAxiomAnnotations = true
;rigid validation of extractor output
; according to extractor/Configuration.php
validateExtractors = false


logpath = log
;rdfapi_include_dir =  api


; Default extraction processing threads
ProcessingThreads = 1

;*********************
; OAI Configuration
;*********************

localApiURL = http://live.dbpedia.org/syncw/api.php

oaiUri = http://live.dbpedia.org/syncwiki/Special:OAIRepository
oaiPrefix = oai:live.dbpedia.org:dbpediawiki:
baseWikiUri = http://live.dbpedia.org/syncwiki/

mappingsOAIUri = http://mappings.dbpedia.org/index.php/Special:OAIRepository
mappingsOaiPrefix = oai:en.wikipedia.org:enwiki:
mappingsBaseWikiUri = http://mappings.dbpedia.org/wiki/

;*********************
; FEEDERS
;*********************

feeder.rcstream.enabled = false
feeder.rcstream.room = en.wikipedia.org
; Specify the namespace code of events you want to be processed
; Full list available at https://en.wikipedia.org/wiki/Wikipedia:Namespace
; Add at least namespace 6 "File:" to process files on commons.wikimedia.org
feeder.rcstream.allowedNamespaces = 0,10,14
; Specify how often the RCStream should try to reconnect (maxRetryCount)
; within a intervall of x minutes (maxRetryCountIntervall)
feeder.rcstream.maxRetryCount = 3
feeder.rcstream.maxRetryCountIntervall = 5

feeder.allpages.enabled = false
feeder.allpages.allowedNamespaces = 0,10,14

feeder.live.enabled = false
feeder.live.pollInterval = 3000
feeder.live.sleepInterval = 1000

feeder.mappings.enabled = false
feeder.mappings.pollInterval = 2000
feeder.mappings.sleepInterval = 1000

feeder.unmodified.enabled = true
feeder.unmodified.pollInterval = 2000
feeder.unmodified.sleepInterval = 1000

feeder.unmodified.minDaysAgo = 30
feeder.unmodified.chunk = 5000
feeder.unmodified.threshold = 500
feeder.unmodified.sleepTime = 30000

feeder.eventstreams.enabled = true
feeder.eventstreams.allowedNamespaces = 0,10,14
feeder.eventstreams.maxLineSize = 32768
feeder.eventstreams.maxEventSize = 65536
; see https://stream.wikimedia.org/?doc for documentation of the EventStreams API
feeder.eventstreams.baseURL = https://stream.wikimedia.org/v2/stream/
feeder.eventstreams.streams = recentchange
;sleeptime in milliseconds
feeder.eventstreams.sleepTime = 3000
feeder.eventstreams.minBackoffFactor = 2
feeder.eventstreams.maxBackoffFactor = 30

;*********************
; OPTIONS FOR LANGUAGE
;*********************

; URI Policy
uri-policy.main = uri:en; generic:en; xml-safe-predicates:*

;the language option might be included, but I'm not sure about it
language = en

;Option to use IRIs instead of URIs (default false)
language_use_IRI = false

;for english, the default
dbpedia_ns = http://dbpedia.org/
db_meta_ns = http://dbpedia.org/meta/

;example for german, might still be changed
;dbpedia_ns = http://de.dbpedia.org/

graphURI = http://live.nl.dbpedia.org

;******************************************
;Below configuration for the live extraction
;******************************************

;show the configuration of extractors at startup
;delays start for 5 seconds
showconfig = false

cache.class = com.mysql.jdbc.Driver
cache.dsn   = jdbc:mysql://localhost/dbpedia_live_cache
cache.user  = root
cache.pw    = mysqlPass


;dryRun doesn't update the store, but instead prints out the sparul
dryRun = false
;additionally the sparul can be written to files
;if you want files only turn dryRun to true
;doesnot have any effect currently
writeSPARULtoFiles = false
outputdirs[] = files/SPARUL

;********Statistics, can't live without it ************
;print statistics after n pages
printStatInterval = 51
;dir with index.html for statistic
statisticdir = files/statistic
useGnuplot = false
harvester_gnu_script = files/harvester_throughput.gnu
;for statistic html to generate links
linkeddataresourceprefix = http://dbpedia2.openlinksw.com:8895/resource/

;********INPUT***********
; in seconds
sleepinterval = 5
currentArticleFile = files/current.record
;this is where new articles should be placed by the harvester
; default : oaiRecords = liveextraction/oairecords
oaiRecords = liveextraction/oairecords
;if there are many files in the oairecord dir use true here
fastFileHandling = true
;turns of article count, because it is slow, if lots of articles are queued
noglob = false


;**********************************
;**OPTIONS FOR PUBLISHING UPDATES**
;**********************************
osmReplicationConfigPath = ./live
tmpPath = /tmp/lgd
sleepInterval = 60
sequenceNumber = 1
;***********************************


;***********************************
; OPTIONS FOR PUBLISHING STATISTICS*
;***********************************
statisticsFilePath = /path/to/statistics/data
