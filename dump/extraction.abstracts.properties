# DBpedia abstract extraction is a rather complex process.
# See the following page for some tips, or ask on the
# dbpedia-discussion or dbpedia-developers mailing lists.
# http://git.io/DBpedia-Abstract-Extraction

# download and extraction target dir
# base-dir=see universal.properties

# Source file. If source file name ends with .gz or .bz2, it is unzipped on the fly. 
# Must exist in the directory xxwiki/yyyymmdd and have the prefix xxwiki-yyyymmdd-
# where xx is the wiki code and yyyymmdd is the dump date.
 
# default:
# source=pages-articles.xml.bz2

# alternatives:
# source=pages-articles.xml.gz
# source=pages-articles.xml

# use only directories that contain a 'download-complete' file? Default is false.
require-download-complete=true

# List of languages or article count ranges, e.g. 'en,de,fr' or '10000-20000' or '10000-', or '@mappings'
languages=@downloaded

# default namespaces: Main, File, Category, Template
# we only want abstracts for articles -> only main namespace
namespaces=Main

# extractor class names starting with "." are prefixed by "org.dbpedia.extraction.mappings"

extractors=.PlainAbstractExtractor

# if ontology and mapping files are not given or do not exist, download info from mappings.dbpedia.org
# ontology=../ontology.xml see universal.properties
# mappings=../mappings see universal.properties

# Serialization URI policies and file formats. Quick guide:
# uri-policy keys: uri, generic, xml-safe, reject-long
# uri-policy position modifiers: -subjects, -predicates, -objects, -datatypes, -contexts
# uri-policy values: comma-separated languages or '*' for all languages
# format values: n-triples, n-quads, turtle-triples, turtle-quads, trix-triples, trix-quads
# See http://git.io/DBpedia-serialization-format-properties for details.

# For backwards compatibility, en uses generic URIs. All others use local IRIs.
# uri-policy.uri=uri:en; generic:en; xml-safe-predicates:*
uri-policy.iri=generic:en; xml-safe-predicates:*

# NT is unreadable anyway - might as well use URIs for en
# format.nt.gz=n-triples;uri-policy.uri
# format.nq.gz=n-quads;uri-policy.uri

# Turtle is much more readable - use nice IRIs for all languages
format.ttl.bz2=turtle-triples;uri-policy.iri
format.tql.bz2=turtle-quads;uri-policy.iri

#the following parameters are for the mediawiki api connection used in nif and abstract extraction

mwc-apiMWCUrl=https://{{LANG}}.wikipedia.org/w/api.php
mwc-apiRestUrl=https://{{LANG}}.wikipedia.org/api/rest_v1/page/html/
mwc-apiLocalUrl=http://localhost:8080/api.php
# chose "rest", "mwc" or "local"
mwc-type=mwc
# MWC params
mwc-maxRetries=5
mwc-connectMs=4000
mwc-readMs=30000
mwc-sleepFactor=2000
# MWC rest params
mwc-maxlag=3
mwc-useragent=(https://dbpedia.org/; dbpedia@infai.org) DIEF
mwc-gzip=true
mwc-retryafter=true
# CONFIG OF REST API
mwc-accept=text/html
mwc-charset=utf-8
mwc-profile=https://www.mediawiki.org/wiki/Specs/HTML/2.1.0


#parameters specific for the abstract extraction
abstract-query=&format=xml&action=query&prop=extracts&exintro=&explaintext=&titles=%s
# the tag path of the XML tags under which the result is expected
abstract-tags=query,pages,page,extract
# the properties used to specify long- and short abstracts (should not change)
short-abstracts-property=rdfs:comment
long-abstracts-property=abstract
# the short abstract is at least this long
short-abstract-min-length=200