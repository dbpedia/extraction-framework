# NOTE: format is not java.util.Properties, but org.dbpedia.extraction.dump.download.DownloadConfig

# Default download server. It lists mirrors which may be faster. 
base-url=http://dumps.wikimedia.org/

# Replace by your target folder.
base-dir=/path/to/download/folder

# Replace xx by your language.
download=xx:pages-articles.xml.bz2

###### Download part files ######
#
# Please make sure that the regex actually matches the format used for xx dumps
# by checking http://dumps.wikimedia.org/xxwiki/yyyymmdd
#
# Example:
# enwiki => enwiki-20131120-pages-articles1.xml-p000000010p000010000.bz2 hence @pages-articles\d+\.xml-p\d+p\d+\.bz2 matches
# frwiki => frwiki-20131120-pages-articles1.xml.bz2 hence @pages-articles\d+\.xml\.bz2 matches (the previous regex does not!)
# commonswiki => it does not have part files! This is true for other wikis as well. In this case xx:pages-articles.xml.bz2
# shoud be used (e.g. commons:pages-articles.xml.bz2 or cowiki:pages-articles.xml.bz2)
#
# download=xx:@pages-articles\d+\.xml-p\d+p\d+\.bz2

# Only needed for the ImageExtractor
# download=commons:pages-articles.xml.bz2

# Unzip files while downloading? Not necessary, extraction will unzip on the fly. Let's save space.
unzip=false

# Sometimes connecting to the server fails, so we try five times with pauses of 10 seconds.
retry-max=5
retry-millis=10000
